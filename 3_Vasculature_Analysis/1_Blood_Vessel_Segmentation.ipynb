{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16c6ed-0edf-42f1-93a4-89b60b7c5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Data\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import tifffile\n",
    "\n",
    "# Architecture imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import List, Tuple\n",
    "import shutil\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "from skimage import exposure\n",
    "import warnings\n",
    "from skimage.morphology import disk, binary_closing,binary_opening\n",
    "from scipy import ndimage\n",
    "import cc3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9582ce8-f1a4-481c-929f-57a787913298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is there\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available for execution \")\n",
    "else:\n",
    "    print(\"GPU not available ! . Please check or proceed to execute in CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759405c-464f-4f0a-bc2c-260358bb48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(r\"sample_test_Data/Input\")\n",
    "output_path = r\"sample_test_Data/Output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27846d-bb89-4e3e-af40-64ee11dc1fbb",
   "metadata": {},
   "source": [
    "## Check Model Presence or Download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c7e03-ec6e-4e00-b2fb-b713c21dbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR=r\"BV_models\"\n",
    "ZIP_URL = \"https://zenodo.org/records/15092730/files/BV_models.zip?download=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c64ea-cf7b-4147-9fb3-8e9231c8a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model folder check\n",
    "def download_and_extract_zip(url: str, extract_to: str):\n",
    "    print(f\"Downloading model zip from {url}...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # raise error if download fails\n",
    "\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "        print(f\"Extracting models to '{extract_to}'...\")\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Models downloaded and extracted successfully.\")\n",
    "\n",
    "def ensure_model_dir():\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "        download_and_extract_zip(ZIP_URL, MODEL_DIR)\n",
    "    else:\n",
    "        print(f\"Model directory '{MODEL_DIR}' already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556f0a0-cd07-4723-be02-ec8c935ec838",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_model_dir() # If model files are not there it will begin download automatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492a872-5ba7-47ad-8f24-297c4ab49003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model environment\n",
    "os.environ[\"nnUNet_raw\"] = \"BV_models/BV_models/nnUNet_raw\"\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"BV_models/BV_models/nnUNet_preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"BV_models/BV_models/nnUNet_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285828ed-bd83-4840-8187-3afbc50f6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnunet details\n",
    "dataset_num=111\n",
    "config=\"3d_fullres\"\n",
    "on_demand=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfe3f4-ec9e-4b3c-9bbd-3b8c701e796f",
   "metadata": {},
   "source": [
    "## Chunk the Image and Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c009355-3560-4cce-ad93-ea9f2e2f2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chuncks(volume, output_folder, tif_file,chunk_size=(128, 256, 256)):\n",
    "    # Calculate number of chunks in each dimension\n",
    "    chunks_z = int(np.ceil(volume.shape[0] / chunk_size[0]))\n",
    "    chunks_y = int(np.ceil(volume.shape[1] / chunk_size[1]))\n",
    "    chunks_x = int(np.ceil(volume.shape[2] / chunk_size[2]))\n",
    "    chunk_num=0\n",
    "\n",
    "    for z in range(chunks_z):\n",
    "        for y in range(chunks_y):\n",
    "            for x in range(chunks_x):\n",
    "                # Calculate chunk boundaries\n",
    "                z_start, z_end = z * chunk_size[0], min((z + 1) * chunk_size[0], volume.shape[0])\n",
    "                y_start, y_end = y * chunk_size[1], min((y + 1) * chunk_size[1], volume.shape[1])\n",
    "                x_start, x_end = x * chunk_size[2], min((x + 1) * chunk_size[2], volume.shape[2])\n",
    "\n",
    "                # Extract chunks\n",
    "                volume_chunk = volume[z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "               \n",
    "                # Pad chunks if necessary\n",
    "                if volume_chunk.shape != chunk_size:\n",
    "                    volume_chunk = np.pad(volume_chunk, \n",
    "                                          ((0, chunk_size[0] - volume_chunk.shape[0]), \n",
    "                                           (0, chunk_size[1] - volume_chunk.shape[1]), \n",
    "                                           (0, chunk_size[2] - volume_chunk.shape[2])),\n",
    "                                          mode='constant')\n",
    "\n",
    "                # Save chunks\n",
    "                chunk_name = f\"{tif_file[:-4]}_z{z}_y{y}_x{x}_{chunk_num:03}_0000.tif\"\n",
    "                tifffile.imwrite(output_folder / chunk_name, volume_chunk)\n",
    "                chunk_num+=1\n",
    "               \n",
    "    print(\"chunks created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec747e5-9ec7-4b63-b70d-22a0938c9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_volume_bicubic(volume, target_size=(256, 819)):\n",
    "    \"\"\"\n",
    "    Performs bicubic interpolation on a 3D volume array to resize x and y dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    volume : numpy.ndarray\n",
    "        Input 3D volume with shape (z, y, x)\n",
    "    target_size : tuple\n",
    "        Desired output size for (y, x) dimensions, default is (256, 819)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Resized volume with shape (z, 256, 819)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get current dimensions\n",
    "    z_dim, y_dim, x_dim = volume.shape\n",
    "    \n",
    "    # Calculate zoom factors for each dimension\n",
    "    z_factor = 1.0  # Keep z dimension unchanged\n",
    "    y_factor = target_size[0] / y_dim\n",
    "    x_factor = target_size[1] / x_dim\n",
    "    \n",
    "    # Perform bicubic interpolation\n",
    "    # order=3 specifies bicubic interpolation\n",
    "    resized_volume = zoom(volume, (z_factor, y_factor, x_factor), order=3)\n",
    "    \n",
    "    return resized_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b0588-a337-47a2-94ab-a7ef99dbfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_label_volume(label_volume, original_shape):\n",
    "    \"\"\"\n",
    "    Resizes a label volume back to its original dimensions using nearest neighbor interpolation\n",
    "    to preserve label values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    label_volume : numpy.ndarray\n",
    "        Input label volume with shape (z, 256, 819)\n",
    "    original_shape : tuple\n",
    "        Original shape to restore to (z, y, x)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Restored label volume with original shape\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get current dimensions\n",
    "    z_dim, y_dim, x_dim = label_volume.shape\n",
    "    \n",
    "    # Calculate zoom factors for each dimension\n",
    "    #z_factor = original_shape[0] / z_dim\n",
    "    z_factor = 1\n",
    "    y_factor = original_shape[1] / y_dim\n",
    "    x_factor = original_shape[2] / x_dim\n",
    "    \n",
    "    # Use nearest neighbor interpolation (order=0) to preserve label values\n",
    "    restored_volume = zoom(label_volume, (z_factor, y_factor, x_factor), order=0)\n",
    "    \n",
    "    return restored_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857c965-402a-49de-af0e-af49468649b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_volume(chunks_folder, output_folder, final_shape, chunk_size=(128, 256, 256)):\n",
    "    chunks_folder = Path(chunks_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Group chunks by original filename\n",
    "    chunk_groups = {}\n",
    "    for chunk_file in chunks_folder.glob(\"*.tif\"):\n",
    "        # Parse chunk file name based on the new pattern\n",
    "        original_name, coords = chunk_file.stem.rsplit('_z', 1)\n",
    "        z, yx_chunknum = coords.split('_y')\n",
    "        y, x_chunknum = yx_chunknum.split('_x')\n",
    "        x, chunk_num = x_chunknum.split('_')\n",
    "        \n",
    "        # Convert coordinates and chunk_num to integers\n",
    "        z, y, x = int(z), int(y), int(x)\n",
    "        \n",
    "        # Group chunks by original file name\n",
    "        if original_name not in chunk_groups:\n",
    "            chunk_groups[original_name] = []\n",
    "        chunk_groups[original_name].append((z, y, x, chunk_file))\n",
    "\n",
    "    for original_name, chunks in chunk_groups.items():\n",
    "        # Determine the shape of the padded volume\n",
    "        max_z = max(chunk[0] for chunk in chunks) + 1\n",
    "        max_y = max(chunk[1] for chunk in chunks) + 1\n",
    "        max_x = max(chunk[2] for chunk in chunks) + 1\n",
    "\n",
    "        # Initialize the reconstructed volume (padded)\n",
    "        padded_shape = (\n",
    "            max_z * chunk_size[0],\n",
    "            max_y * chunk_size[1],\n",
    "            max_x * chunk_size[2]\n",
    "        )\n",
    "        reconstructed_volume = np.zeros(padded_shape, dtype=np.float32)\n",
    "\n",
    "        # Fill the reconstructed volume with chunks\n",
    "        for z, y, x, chunk_file in chunks:\n",
    "            chunk = tifffile.imread(chunk_file)\n",
    "            reconstructed_volume[\n",
    "                z * chunk_size[0] : (z + 1) * chunk_size[0],\n",
    "                y * chunk_size[1] : (y + 1) * chunk_size[1],\n",
    "                x * chunk_size[2] : (x + 1) * chunk_size[2]\n",
    "            ] = chunk\n",
    "\n",
    "        # Crop the reconstructed volume to the final shape\n",
    "        #final_volume = reconstructed_volume[:final_shape[0], :final_shape[1], :final_shape[2]]\n",
    "        final_volume = reconstructed_volume\n",
    "\n",
    "        # Save the reconstruction\n",
    "        #output_file = output_folder / f\"{original_name}_reconstructed_original_before_shape_adjustment.tif\"\n",
    "        #tifffile.imwrite(output_file, final_volume)\n",
    "\n",
    "        # Adjust the size of the final label\n",
    "        restored_final_volume = restore_label_volume(final_volume, final_shape)\n",
    "\n",
    "        # Unnecessary step - at this stage final shape should be the shaoe if restored_final_volume - but still clipping\n",
    "        restored_final_volume = restored_final_volume[:final_shape[0], :final_shape[1], :final_shape[2]]\n",
    "\n",
    "        # Save the reconstructed and cropped volume\n",
    "        #output_file = output_folder / f\"{original_name}_reconstructed.tif\"\n",
    "        #tifffile.imwrite(output_file, restored_final_volume)\n",
    "\n",
    "    print(\"Reconstruction complete.\")\n",
    "    return restored_final_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676afac-cebd-4e1e-8134-cca9733c8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nnunet(input_path, output_path, dataset_num, config):\n",
    "    # Create output directory\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Run command\n",
    "    cmd = [\n",
    "        \"nnUNetv2_predict\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-o\", str(output_path),\n",
    "        \"-d\", str(dataset_num),\n",
    "        \"-c\", config,\n",
    "        \"--save_probabilities\"\n",
    "    ]\n",
    "\n",
    "    result = ' '.join(cmd)\n",
    "    print(\"command is\", result)\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, text=True)\n",
    "        print(\"Prediction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d4c68-f928-40f0-a920-5ba1128a615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nnunet_ondemand(input_path, output_path, dataset_num, config):\n",
    "    # Create output directory\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Run command\n",
    "    cmd = [\n",
    "        \"nnUNetv2_predict\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-o\", str(output_path),\n",
    "        \"-d\", str(dataset_num),\n",
    "        \"-c\", config,\n",
    "        \"--save_probabilities\"\n",
    "    ]\n",
    "\n",
    "    cmd = [\"conda\", \"run\", \"-p\", \"\"] + cmd\n",
    "\n",
    "    result = ' '.join(cmd)\n",
    "    print(\"command is\", result)\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, text=True)\n",
    "        print(\"Prediction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04aa16f-46fb-46cb-935d-83e0699b3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_for_single_volume(file_path,output_path):\n",
    "    print(\"Executing for \",file_path.stem)\n",
    "\n",
    "    # Read the volume\n",
    "    volume = tifffile.imread(file_path)\n",
    "    \n",
    "    # Make Chuncks\n",
    "    chuncked_volume_output = Path(output_path) / (str(file_path.stem) + \"_chunks\")\n",
    "    chuncked_volume_output.mkdir(parents=True, exist_ok=True)    \n",
    "    make_chuncks(volume, chuncked_volume_output, file_path.name,chunk_size=(100, 819, 819))\n",
    "    \n",
    "    # Create Segmentation\n",
    "    model_outputs = Path(output_path) / (str(file_path.stem) + \"_segmentations\")\n",
    "    model_outputs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Execute nn unet - change here if you running on downsampled image --- for faster execution remove downsampling in future\n",
    "    if on_demand:\n",
    "        run_nnunet_ondemand(chuncked_volume_output, model_outputs, dataset_num, config)\n",
    "        #print(\"\")\n",
    "    else:\n",
    "        run_nnunet(chuncked_volume_output, model_outputs, dataset_num, config)\n",
    "    \n",
    "    # Reconstruct\n",
    "    restored_final_volume = reconstruct_volume(model_outputs, output_path, volume.shape, chunk_size=(100, 819, 819))\n",
    "    return restored_final_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b376ad6-7bcc-43f9-8206-26a892202fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_small_regions(mask, original_image, max_diameter=20, intensity_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Fill holes in mask based on both size and intensity criteria.\n",
    "    \n",
    "    Args:\n",
    "        mask (np.ndarray): Input 3D binary mask\n",
    "        original_image (np.ndarray): Original intensity image\n",
    "        max_diameter (int): Maximum diameter of regions to fill\n",
    "        intensity_threshold (float): Minimum intensity threshold (as percentage)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Processed mask with holes filled based on criteria\n",
    "    \"\"\"\n",
    "    # Get inverse of mask to identify holes\n",
    "    inverse_mask = ~mask\n",
    "    \n",
    "    # Label connected components in the inverse mask (holes)\n",
    "    labeled_holes, num_holes = ndimage.label(inverse_mask)\n",
    "    \n",
    "    # Calculate median intensity of all masked regions\n",
    "    masked_intensities = original_image[mask]\n",
    "    if len(masked_intensities) > 0:  # Check if any masked regions exist\n",
    "        global_median = np.median(masked_intensities)\n",
    "    else:\n",
    "        global_median = 0\n",
    "    \n",
    "    # Initialize output mask\n",
    "    filled_mask = mask.copy()\n",
    "    \n",
    "    # Process each hole\n",
    "    for hole_label in range(1, num_holes + 1):\n",
    "        # Extract current hole\n",
    "        current_hole = labeled_holes == hole_label\n",
    "        \n",
    "        # Compute distance transform of the hole\n",
    "        distances = ndimage.distance_transform_edt(current_hole)\n",
    "        \n",
    "        # Maximum diameter is twice the maximum distance from edge\n",
    "        max_distance = np.max(distances)\n",
    "        diameter = 2 * max_distance\n",
    "        \n",
    "        # Calculate mean intensity of the current hole\n",
    "        hole_intensity = np.mean(original_image[current_hole])\n",
    "        \n",
    "        # Calculate intensity ratio compared to global median\n",
    "        intensity_ratio = hole_intensity / global_median if global_median > 0 else 0\n",
    "        \n",
    "        # Fill hole if it meets both criteria:\n",
    "        # 1. Diameter is less than threshold\n",
    "        # 2. Intensity is at least n% (intensity_threshold) of the global median\n",
    "        if diameter < max_diameter and intensity_ratio >= intensity_threshold:\n",
    "            filled_mask[current_hole] = True\n",
    "            \n",
    "    return filled_mask\n",
    "\n",
    "def make_watertight_with_size_constraint(mask, original_image, max_diameter=20, intensity_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Make a 3D binary mask watertight, filling holes based on size and intensity criteria.\n",
    "    \n",
    "    Args:\n",
    "        mask (np.ndarray): Input 3D binary mask\n",
    "        original_image (np.ndarray): Original intensity image\n",
    "        max_diameter (int): Maximum diameter of regions to fill\n",
    "        intensity_threshold (float): Minimum intensity threshold (as percentage)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Watertight 3D binary mask\n",
    "    \"\"\"\n",
    "    # Create structure element for 3D connectivity\n",
    "    struct = ndimage.generate_binary_structure(3, 1)\n",
    "    \n",
    "    # Step 1: Fill small holes in each 2D slice\n",
    "    filled_2d = np.zeros_like(mask)\n",
    "    for z in range(mask.shape[0]):\n",
    "        slice_mask = mask[z]\n",
    "        slice_image = original_image[z]\n",
    "        filled_2d[z] = fill_small_regions(slice_mask, slice_image, max_diameter, intensity_threshold)\n",
    "    \n",
    "    # Step 2: Close small gaps\n",
    "    closed = ndimage.binary_closing(filled_2d, structure=struct, iterations=1)\n",
    "    \n",
    "    # Step 3: Fill small holes in 3D for all components\n",
    "    final_mask = fill_small_regions(closed, original_image, max_diameter, intensity_threshold)\n",
    "    \n",
    "    return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84d0c4-0e58-4830-9b6b-0541ee7f4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mask_with_size_constraint(mask_filepath, image_filepath, max_diameter=20, intensity_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Process a 3D mask file and make it watertight, filling holes based on size and intensity.\n",
    "    \n",
    "    Args:\n",
    "        mask_filepath (str): Path to input mask TIFF file\n",
    "        image_filepath (str): Path to original intensity image TIFF file\n",
    "        max_diameter (int): Maximum diameter of regions to fill\n",
    "        intensity_threshold (float): Minimum intensity threshold (as percentage)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Processed watertight 3D mask\n",
    "    \"\"\"\n",
    "    # Read mask and original image\n",
    "    mask = tifffile.imread(mask_filepath)\n",
    "    original_image = tifffile.imread(image_filepath)\n",
    "    \n",
    "    # Ensure mask is binary\n",
    "    if mask.dtype != bool:\n",
    "        mask = mask > 0\n",
    "    \n",
    "    # Make watertight with size and intensity constraints\n",
    "    watertight = make_watertight_with_size_constraint(\n",
    "        mask, \n",
    "        original_image, \n",
    "        max_diameter, \n",
    "        intensity_threshold\n",
    "    )\n",
    "    \n",
    "    return watertight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae4a9f-8a49-4862-98bd-f89dddd37faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphological_opening_3d_slices(binary_mask, disk_size=3):\n",
    "    \"\"\"\n",
    "    Perform morphological closing on a 3D binary mask slice by slice using disk structure.\n",
    "    \n",
    "    Args:\n",
    "        binary_mask (numpy.ndarray): 3D binary array\n",
    "        disk_size (int): Diameter of the disk structure\n",
    "    Returns:\n",
    "        numpy.ndarray: Processed binary mask\n",
    "    \"\"\"\n",
    "    # Create disk structure using skimage\n",
    "    structure = disk(disk_size // 2)  # radius = diameter/2\n",
    "    \n",
    "    output = np.zeros_like(binary_mask)\n",
    "    for i in range(binary_mask.shape[0]):\n",
    "        output[i] = binary_opening(binary_mask[i], structure)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49af3f-9e9e-4692-a9f6-197040d9be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_objects_by_volume(mask, min_volume=None, max_volume=None):\n",
    "    \"\"\"\n",
    "    Filter objects in a 3D binary mask based on their volume using connected components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mask : numpy.ndarray\n",
    "        3D binary array where objects are marked with 1s and background with 0s\n",
    "    min_volume : int or None\n",
    "        Minimum volume threshold. Objects smaller than this will be removed.\n",
    "        If None, no minimum threshold is applied.\n",
    "    max_volume : int or None\n",
    "        Maximum volume threshold. Objects larger than this will be removed.\n",
    "        If None, no maximum threshold is applied.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Filtered binary mask with same shape as input, where only objects\n",
    "        meeting the volume criteria remain\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run connected components\n",
    "    labels = cc3d.connected_components(mask, connectivity=6)\n",
    "    \n",
    "    # Get volume of each component\n",
    "    stats = cc3d.statistics(labels)\n",
    "    volumes = stats['voxel_counts'][1:]  # Skip background (label 0)\n",
    "    \n",
    "    # Create mask for valid objects\n",
    "    valid_objects = np.ones(len(volumes), dtype=bool)\n",
    "    \n",
    "    # Apply minimum volume threshold\n",
    "    if min_volume is not None:\n",
    "        valid_objects &= volumes >= min_volume\n",
    "        \n",
    "    # Apply maximum volume threshold\n",
    "    if max_volume is not None:\n",
    "        valid_objects &= volumes <= max_volume\n",
    "    \n",
    "    # Create output mask\n",
    "    valid_labels = np.nonzero(valid_objects)[0] + 1  # Add 1 since we skipped background\n",
    "    output_mask = np.isin(labels, valid_labels)\n",
    "    \n",
    "    return output_mask.astype(mask.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd59f8-a179-4e2a-98e7-e4e9a4fcda14",
   "metadata": {},
   "source": [
    "## Get all valid paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965208e7-d063-4f4a-93d6-66148b8c154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_icam2_paths(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process directories to find Icam2 images and create corresponding result folders.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to the input directory containing processed images\n",
    "        output_dir (str): Path to create DAPI results folders\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Lists of (input DAPI paths, output result folder paths)\n",
    "    \"\"\"\n",
    "    # Initialize lists to store paths\n",
    "    icam2_paths = []\n",
    "    dapi_paths = []\n",
    "    icam2_result_paths = []\n",
    "    \n",
    "    # Convert to Path objects for easier handling\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        # Convert current root to Path object\n",
    "        root_path = Path(root)\n",
    "        \n",
    "        # Check if we're in an isotropic_image folder\n",
    "        if root_path.name == \"isotropic_image\":\n",
    "            file_name = \"C1-Icam2-Blood-Vessels.tif\"\n",
    "            # dapi_file_name = \"C4-DAPI-XZ.tif\"\n",
    "\n",
    "            # Look for C4-DAPI-XZ.tif in files\n",
    "            if file_name in files:\n",
    "                # Get the full path to the DAPI XZ image\n",
    "                full_path = root_path / file_name\n",
    "                # adpi_path = root_path / dapi_file_name\n",
    "                \n",
    "                # Get the series folder name (parent of isotropic_image)\n",
    "                series_folder = root_path.parent.name\n",
    "                \n",
    "                # Create corresponding output folder structure\n",
    "                result_folder = output_path / series_folder / \"ICAM2_results\"\n",
    "                result_folder.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Add paths to lists\n",
    "                icam2_paths.append(str(full_path))\n",
    "                # dapi_path = root_path / dapi_file_name\n",
    "                icam2_result_paths.append(str(result_folder))\n",
    "                \n",
    "                print(f\"Found  ICAM2 image: {full_path}\")\n",
    "                # print(f\"Found DAPI image: {dapi_path}\")\n",
    "                print(f\"Created results folder: {result_folder}\")\n",
    "    \n",
    "    return icam2_paths, icam2_result_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d04db9-2aa9-4568-bbf3-fb3f508e7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_paths, result_paths = process_icam2_paths(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cae0e0-7181-4518-8de5-23944e892b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bv_paths))\n",
    "print(bv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884826f-a155-42a4-b08b-569348666acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bv_input_file, result_output_directory in zip(bv_paths, result_paths):\n",
    "    print(\"Executing BV  - model segmentation for :\")\n",
    "    print(bv_input_file)\n",
    "    # Execute for single volume\n",
    "    restored_final_volume = execute_for_single_volume(Path(bv_input_file),result_output_directory)\n",
    "\n",
    "    # Save the nnunet output\n",
    "    mask_output_file = Path(result_output_directory) / \"C1-Icam2-Blood-Vessels_nnunet_reconstructed.tif\"\n",
    "    tifffile.imwrite(str(mask_output_file), restored_final_volume)\n",
    "\n",
    "    # Clean up the segmentation\n",
    "    # Process mask with size and intensity constraints\n",
    "    opened_filtered = morphological_opening_3d_slices(restored_final_volume,disk_size=2)\n",
    "    opened_output_file = Path(result_output_directory) / \"C1-Icam2-Blood-Vessels_nnunet_reconstructed_opened.tif\"\n",
    "    tifffile.imwrite(str(opened_output_file), opened_filtered)\n",
    "    \n",
    "    result = process_mask_with_size_constraint(opened_output_file,bv_input_file,max_diameter=20,intensity_threshold=0.05)\n",
    "\n",
    "    # Perform volume filtering\n",
    "    volume_filtered = filter_objects_by_volume(result, min_volume=1000)\n",
    "    \n",
    "    # Save\n",
    "    output_file = Path(result_output_directory) / \"C1-Icam2-Blood-Vessels_nnunet_reconstructed_cleaned_filtered.tif\"\n",
    "    tifffile.imwrite(str(output_file), volume_filtered)\n",
    "\n",
    "print(\"Processing complete !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aed7c5-35ec-4ebc-b947-53cd78d49893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff432e46-19ee-40b5-9b48-cfbaf00db6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7cc63-957b-428d-a365-8fb5b8ed4dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
