{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae16c6ed-0edf-42f1-93a4-89b60b7c5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Data\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import tifffile\n",
    "\n",
    "# Architecture imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import List, Tuple\n",
    "import shutil\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "from skimage import exposure\n",
    "import warnings\n",
    "\n",
    "import cc3d\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9582ce8-f1a4-481c-929f-57a787913298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available for execution \n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is there\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available for execution \")\n",
    "else:\n",
    "    print(\"GPU not available ! . Please check or proceed to execute in CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6559684-0b70-45c4-b7e4-1367d2e18358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"nnUNet_raw\"] = \"DAPI_models/nnUNet_raw\"\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"DAPI_models/nnUNet_preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"DAPI_models/nnUNet_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a759405c-464f-4f0a-bc2c-260358bb48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(r\"/research/sharedresources/cbi/common/Krishnan/Sickle_cell_git_repo/sample_test_Data/Input\")\n",
    "output_path = r\"/research/sharedresources/cbi/common/Krishnan/Sickle_cell_git_repo/sample_test_Data/Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062f8038-bc18-448e-8f05-05a4af23c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnunet details\n",
    "dataset_num=104\n",
    "config=\"3d_fullres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0b2090-f227-4665-b920-5d4938fef947",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_demand=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfe3f4-ec9e-4b3c-9bbd-3b8c701e796f",
   "metadata": {},
   "source": [
    "## Chunk the Image and Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c009355-3560-4cce-ad93-ea9f2e2f2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chuncks(volume, output_folder, tif_file,chunk_size=(128, 256, 256)):\n",
    "    # Calculate number of chunks in each dimension\n",
    "    chunks_z = int(np.ceil(volume.shape[0] / chunk_size[0]))\n",
    "    chunks_y = int(np.ceil(volume.shape[1] / chunk_size[1]))\n",
    "    chunks_x = int(np.ceil(volume.shape[2] / chunk_size[2]))\n",
    "    chunk_num=0\n",
    "\n",
    "    for z in range(chunks_z):\n",
    "        for y in range(chunks_y):\n",
    "            for x in range(chunks_x):\n",
    "                # Calculate chunk boundaries\n",
    "                z_start, z_end = z * chunk_size[0], min((z + 1) * chunk_size[0], volume.shape[0])\n",
    "                y_start, y_end = y * chunk_size[1], min((y + 1) * chunk_size[1], volume.shape[1])\n",
    "                x_start, x_end = x * chunk_size[2], min((x + 1) * chunk_size[2], volume.shape[2])\n",
    "\n",
    "                # Extract chunks\n",
    "                volume_chunk = volume[z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "               \n",
    "                # Pad chunks if necessary\n",
    "                if volume_chunk.shape != chunk_size:\n",
    "                    volume_chunk = np.pad(volume_chunk, \n",
    "                                          ((0, chunk_size[0] - volume_chunk.shape[0]), \n",
    "                                           (0, chunk_size[1] - volume_chunk.shape[1]), \n",
    "                                           (0, chunk_size[2] - volume_chunk.shape[2])),\n",
    "                                          mode='constant')\n",
    "\n",
    "                # Save chunks\n",
    "                chunk_name = f\"{tif_file[:-4]}_z{z}_y{y}_x{x}_{chunk_num:03}_0000.tif\"\n",
    "                tifffile.imwrite(output_folder / chunk_name, volume_chunk)\n",
    "                chunk_num+=1\n",
    "               \n",
    "    print(\"chunks created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec747e5-9ec7-4b63-b70d-22a0938c9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_volume_bicubic(volume, target_size=(256, 819)):\n",
    "    \"\"\"\n",
    "    Performs bicubic interpolation on a 3D volume array to resize x and y dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    volume : numpy.ndarray\n",
    "        Input 3D volume with shape (z, y, x)\n",
    "    target_size : tuple\n",
    "        Desired output size for (y, x) dimensions, default is (256, 819)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Resized volume with shape (z, 256, 819)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get current dimensions\n",
    "    z_dim, y_dim, x_dim = volume.shape\n",
    "    \n",
    "    # Calculate zoom factors for each dimension\n",
    "    z_factor = 1.0  # Keep z dimension unchanged\n",
    "    y_factor = target_size[0] / y_dim\n",
    "    x_factor = target_size[1] / x_dim\n",
    "    \n",
    "    # Perform bicubic interpolation\n",
    "    # order=3 specifies bicubic interpolation\n",
    "    resized_volume = zoom(volume, (z_factor, y_factor, x_factor), order=3)\n",
    "    \n",
    "    return resized_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094b0588-a337-47a2-94ab-a7ef99dbfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_label_volume(label_volume, original_shape):\n",
    "    \"\"\"\n",
    "    Resizes a label volume back to its original dimensions using nearest neighbor interpolation\n",
    "    to preserve label values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    label_volume : numpy.ndarray\n",
    "        Input label volume with shape (z, 256, 819)\n",
    "    original_shape : tuple\n",
    "        Original shape to restore to (z, y, x)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Restored label volume with original shape\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get current dimensions\n",
    "    z_dim, y_dim, x_dim = label_volume.shape\n",
    "    \n",
    "    # Calculate zoom factors for each dimension\n",
    "    #z_factor = original_shape[0] / z_dim\n",
    "    z_factor = 1\n",
    "    y_factor = original_shape[1] / y_dim\n",
    "    x_factor = original_shape[2] / x_dim\n",
    "    \n",
    "    # Use nearest neighbor interpolation (order=0) to preserve label values\n",
    "    restored_volume = zoom(label_volume, (z_factor, y_factor, x_factor), order=0)\n",
    "    \n",
    "    return restored_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6fc748-fcfe-42ca-98e3-6c93337c5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clahe_3d(volume, kernel_size=(8, 8), clip_limit=0.01, nbins=256):\n",
    "    \"\"\"\n",
    "    Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to a 3D volume\n",
    "    slice by slice along the z-axis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    volume : numpy.ndarray\n",
    "        Input 3D volume with shape (z, y, x)\n",
    "    kernel_size : tuple\n",
    "        Size of kernel for CLAHE in (y, x) dimensions, default is (8, 8)\n",
    "    clip_limit : float\n",
    "        Clipping limit for CLAHE, normalized between 0 and 1\n",
    "    nbins : int\n",
    "        Number of bins for histogram, default is 256\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        CLAHE processed volume with same shape as input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if volume.ndim != 3:\n",
    "        raise ValueError(\"Input volume must be 3D\")\n",
    "        \n",
    "    # Convert to float and normalize to [0, 1] if not already\n",
    "    if volume.dtype != np.float32 and volume.dtype != np.float64:\n",
    "        volume_norm = volume.astype(float)\n",
    "        if volume_norm.max() > 1.0:\n",
    "            volume_norm = (volume_norm - volume_norm.min()) / (volume_norm.max() - volume_norm.min())\n",
    "    else:\n",
    "        volume_norm = volume.copy()\n",
    "    \n",
    "    # Initialize CLAHE object\n",
    "    clahe = exposure.equalize_adapthist\n",
    "    \n",
    "    # Process each slice\n",
    "    processed_volume = np.zeros_like(volume_norm)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for z in range(volume.shape[0]):\n",
    "            processed_volume[z] = clahe(\n",
    "                volume_norm[z],\n",
    "                kernel_size=kernel_size,\n",
    "                clip_limit=clip_limit,\n",
    "                nbins=nbins\n",
    "            )\n",
    "    \n",
    "    return processed_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "640c40b2-7e06-4e13-9a99-eae5ffa862c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xz_to_xy(volume):\n",
    "    return np.transpose(volume, (1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2857c965-402a-49de-af0e-af49468649b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_volume(chunks_folder, output_folder, final_shape, chunk_size=(128, 256, 256)):\n",
    "    chunks_folder = Path(chunks_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Final shape should be\",final_shape)\n",
    "    print(\"Final shape should be\",final_shape[1])\n",
    "\n",
    "    # Group chunks by original filename\n",
    "    chunk_groups = {}\n",
    "    for chunk_file in chunks_folder.glob(\"*.tif\"):\n",
    "        # Parse chunk file name based on the new pattern\n",
    "        original_name, coords = chunk_file.stem.rsplit('_z', 1)\n",
    "        z, yx_chunknum = coords.split('_y')\n",
    "        y, x_chunknum = yx_chunknum.split('_x')\n",
    "        x, chunk_num = x_chunknum.split('_')\n",
    "        \n",
    "        # Convert coordinates and chunk_num to integers\n",
    "        z, y, x = int(z), int(y), int(x)\n",
    "        \n",
    "        # Group chunks by original file name\n",
    "        if original_name not in chunk_groups:\n",
    "            chunk_groups[original_name] = []\n",
    "        chunk_groups[original_name].append((z, y, x, chunk_file))\n",
    "\n",
    "    for original_name, chunks in chunk_groups.items():\n",
    "        # Determine the shape of the padded volume\n",
    "        max_z = max(chunk[0] for chunk in chunks) + 1\n",
    "        max_y = max(chunk[1] for chunk in chunks) + 1\n",
    "        max_x = max(chunk[2] for chunk in chunks) + 1\n",
    "\n",
    "        # Initialize the reconstructed volume (padded)\n",
    "        padded_shape = (\n",
    "            max_z * chunk_size[0],\n",
    "            max_y * chunk_size[1],\n",
    "            max_x * chunk_size[2]\n",
    "        )\n",
    "        reconstructed_volume = np.zeros(padded_shape, dtype=np.float32)\n",
    "\n",
    "        # Fill the reconstructed volume with chunks\n",
    "        for z, y, x, chunk_file in chunks:\n",
    "            chunk = tifffile.imread(chunk_file)\n",
    "            reconstructed_volume[\n",
    "                z * chunk_size[0] : (z + 1) * chunk_size[0],\n",
    "                y * chunk_size[1] : (y + 1) * chunk_size[1],\n",
    "                x * chunk_size[2] : (x + 1) * chunk_size[2]\n",
    "            ] = chunk\n",
    "\n",
    "        # Crop the reconstructed volume to the final shape\n",
    "        #final_volume = reconstructed_volume[:final_shape[0], :final_shape[1], :final_shape[2]]\n",
    "        final_volume = reconstructed_volume\n",
    "\n",
    "        # Save the reconstruction\n",
    "        output_file = output_folder / f\"{original_name}_reconstructed_original_before_shape_adjustment.tif\"\n",
    "        tifffile.imwrite(output_file, final_volume)\n",
    "\n",
    "        # Adjust the size of the final label\n",
    "        if final_shape[1] > 256:\n",
    "            print(\"Interpolating back to original size\")\n",
    "            restored_final_volume = restore_label_volume(final_volume, final_shape)\n",
    "        else:\n",
    "            restored_final_volume = final_volume # No need to perform reverse interpolation\n",
    "        \n",
    "        # Unnecessary step - at this stage final shape should be the shaoe if restored_final_volume - but still clipping - useful for non interpolation cases\n",
    "        restored_final_volume = restored_final_volume[:final_shape[0], :final_shape[1], :final_shape[2]]\n",
    "\n",
    "        # Save the reconstructed and cropped volume\n",
    "        output_file = output_folder / f\"{original_name}_reconstructed.tif\"\n",
    "        tifffile.imwrite(output_file, restored_final_volume)\n",
    "\n",
    "        # Transpose from xz to xy and save\n",
    "        xy_final_volume = convert_xz_to_xy(restored_final_volume)\n",
    "        xy_output_file = output_folder / f\"{original_name}_reconstructed_xy.tif\"\n",
    "        tifffile.imwrite(xy_output_file, xy_final_volume)\n",
    "        \n",
    "\n",
    "    print(\"Reconstruction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b676afac-cebd-4e1e-8134-cca9733c8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nnunet(input_path, output_path, dataset_num, config):\n",
    "    # Create output directory\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Run command\n",
    "    cmd = [\n",
    "        \"nnUNetv2_predict\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-o\", str(output_path),\n",
    "        \"-d\", str(dataset_num),\n",
    "        \"-c\", config,\n",
    "        \"--save_probabilities\"\n",
    "    ]\n",
    "\n",
    "    result = ' '.join(cmd)\n",
    "    print(\"command is\", result)\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, text=True)\n",
    "        print(\"Prediction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6d4c68-f928-40f0-a920-5ba1128a615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nnunet_ondemand(input_path, output_path, dataset_num, config):\n",
    "    # Create output directory\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Run command\n",
    "    cmd = [\n",
    "        \"nnUNetv2_predict\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-o\", str(output_path),\n",
    "        \"-d\", str(dataset_num),\n",
    "        \"-c\", config,\n",
    "        \"--save_probabilities\"\n",
    "    ]\n",
    "\n",
    "    cmd = [\"conda\", \"run\", \"-p\", \"/research/sharedresources/cbi/public/conda_envs/nnunet\"] + cmd\n",
    "\n",
    "    result = ' '.join(cmd)\n",
    "    print(\"command is\", result)\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, text=True)\n",
    "        print(\"Prediction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04aa16f-46fb-46cb-935d-83e0699b3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_for_single_volume(file_path,output_path):\n",
    "    print(\"Executing for \",file_path.stem)\n",
    "\n",
    "    # Read the volume\n",
    "    volume = tifffile.imread(file_path)\n",
    "\n",
    "    z_dim,y_dim,x_dim = volume.shape\n",
    "\n",
    "    # Restructure the input volume to match nn input shape\n",
    "    if y_dim > 256:\n",
    "        print(\"Resizing Volume since dimension is greater than 256\")\n",
    "        volume = resize_volume_bicubic(volume, target_size=(256, 819))\n",
    "\n",
    "    #print(\"Preprocessing with clahe\")\n",
    "    # Apply clahe \n",
    "    #resized_preprocessed_volume = apply_clahe_3d(resized_volume)\n",
    "    \n",
    "    # Make Chuncks\n",
    "    chuncked_volume_output = Path(output_path) / (str(file_path.stem) + \"_chunks\")\n",
    "    chuncked_volume_output.mkdir(parents=True, exist_ok=True)    \n",
    "    make_chuncks(volume, chuncked_volume_output, file_path.name,chunk_size=(128, 256, 819))\n",
    "    \n",
    "    # Create Segmentation\n",
    "    model_outputs = Path(output_path) / (str(file_path.stem) + \"_segmentations\")\n",
    "    model_outputs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Execute nn unet - change here if you running on downsampled image --- for faster execution remove downsampling in future\n",
    "    if on_demand:\n",
    "        run_nnunet_ondemand(chuncked_volume_output, model_outputs, dataset_num, config)\n",
    "        #print(\"\")\n",
    "    else:\n",
    "        run_nnunet(chuncked_volume_output, model_outputs, dataset_num, config)\n",
    "    \n",
    "    # Reconstruct\n",
    "    #reconstruct_volume(model_outputs_upsampled, output_path, volume.shape, chunk_size=(128, 256, 256))\n",
    "    #reconstruct_volume(model_outputs, output_path, volume.shape, chunk_size=(128, 256, 256))\n",
    "    reconstruct_volume(model_outputs, output_path, (z_dim,y_dim,x_dim), chunk_size=(128, 256, 819))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd59f8-a179-4e2a-98e7-e4e9a4fcda14",
   "metadata": {},
   "source": [
    "## Get all valid isotropic DAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965208e7-d063-4f4a-93d6-66148b8c154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dapi_paths(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process directories to find DAPI XZ images and create corresponding result folders.\n",
    "    Skip folders that already contain processed results (C4-DAPI-XZ_reconstructed.tif).\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to the input directory containing processed images\n",
    "        output_dir (str): Path to create DAPI results folders\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Lists of (input DAPI paths, output result folder paths)\n",
    "    \"\"\"\n",
    "    # Initialize lists to store paths\n",
    "    dapi_xz_paths = []\n",
    "    dapi_result_paths = []\n",
    "    \n",
    "    # Convert to Path objects for easier handling\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        # Convert current root to Path object\n",
    "        root_path = Path(root)\n",
    "        \n",
    "        # Check if we're in an isotropic_image folder\n",
    "        if root_path.name == \"isotropic_image\":\n",
    "            # Look for C4-DAPI-XZ.tif in files\n",
    "            if \"C4-DAPI-XZ.tif\" in files:\n",
    "                # Get the full path to the DAPI XZ image\n",
    "                dapi_path = root_path / \"C4-DAPI-XZ.tif\"\n",
    "                \n",
    "                # Get the series folder name (parent of isotropic_image)\n",
    "                series_folder = root_path.parent.name\n",
    "                \n",
    "                # Create corresponding output folder structure\n",
    "                result_folder = output_path / series_folder / \"DAPI_results\"\n",
    "                result_folder.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Check if reconstructed file already exists\n",
    "                reconstructed_file = result_folder / \"C4-DAPI-XZ_reconstructed.tif\"\n",
    "                if reconstructed_file.exists():\n",
    "                    print(f\"Skipping {dapi_path} - reconstructed file already exists\")\n",
    "                    continue\n",
    "                \n",
    "                # Add paths to lists\n",
    "                dapi_xz_paths.append(str(dapi_path))\n",
    "                dapi_result_paths.append(str(result_folder))\n",
    "                \n",
    "                print(f\"Found DAPI XZ image: {dapi_path}\")\n",
    "                print(f\"Created results folder: {result_folder}\")\n",
    "    \n",
    "    return dapi_xz_paths, dapi_result_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0d04db9-2aa9-4568-bbf3-fb3f508e7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found DAPI XZ image: /research/sharedresources/cbi/common/Krishnan/Sickle_cell_git_repo/sample_test_Data/Input/DL6-2-23-21_Lu-T39ExB_M_AcquisitionBlock2_series1/isotropic_image/C4-DAPI-XZ.tif\n",
      "Created results folder: /research/sharedresources/cbi/common/Krishnan/Sickle_cell_git_repo/sample_test_Data/Output/DL6-2-23-21_Lu-T39ExB_M_AcquisitionBlock2_series1/DAPI_results\n"
     ]
    }
   ],
   "source": [
    "dapi_paths, result_paths = process_dapi_paths(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51cae0e0-7181-4518-8de5-23944e892b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['/research/sharedresources/cbi/common/Krishnan/Sickle_cell_git_repo/sample_test_Data/Input/DL6-2-23-21_Lu-T39ExB_M_AcquisitionBlock2_series1/isotropic_image/C4-DAPI-XZ.tif']\n"
     ]
    }
   ],
   "source": [
    "print(len(dapi_paths))\n",
    "print(dapi_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884826f-a155-42a4-b08b-569348666acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dapi_input_file, result_output_directory in zip(dapi_paths, result_paths):\n",
    "    print(\"Executing DAPI - model segmentation for :\")\n",
    "    print(dapi_input_file)\n",
    "    # Execute for single volume\n",
    "    execute_for_single_volume(Path(dapi_input_file),result_output_directory)\n",
    "print(\"Processing complete !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f801906-9996-442e-a80e-11eea9c93c04",
   "metadata": {},
   "source": [
    "## DAPI Clean UP Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff432e46-19ee-40b5-9b48-cfbaf00db6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_largest_component(volume, label, min_volume_threshold):\n",
    "    \"\"\"\n",
    "    Extract the largest connected component of a given label using cc3d.\n",
    "    Set to 0 if below volume threshold.\n",
    "    \"\"\"\n",
    "    label_mask = (volume == label)\n",
    "    labels_out, N = cc3d.connected_components(label_mask, return_N=True)\n",
    "    \n",
    "    if N == 0:\n",
    "        return volume\n",
    "    \n",
    "    stats = cc3d.statistics(labels_out)\n",
    "    component_sizes = [stats['voxel_counts'][i] for i in range(1, N + 1)]\n",
    "    \n",
    "    output = volume.copy()\n",
    "    output[volume == label] = 0\n",
    "    \n",
    "    if len(component_sizes) > 0:\n",
    "        largest_size = max(component_sizes)\n",
    "        if largest_size >= min_volume_threshold:\n",
    "            largest_component_idx = np.argmax(component_sizes) + 1\n",
    "            output[labels_out == largest_component_idx] = label\n",
    "            \n",
    "    return output\n",
    "\n",
    "def fill_horizontal_zeros(slice_2d):\n",
    "    \"\"\"\n",
    "    Fill zero islands in a 2D slice using horizontal neighbors based on midpoint.\n",
    "    \"\"\"\n",
    "    filled_slice = slice_2d.copy()\n",
    "    y_dim, x_dim = slice_2d.shape\n",
    "    mid_point = x_dim // 2\n",
    "    \n",
    "    for y in range(y_dim):\n",
    "        row = slice_2d[y]\n",
    "        zero_positions = np.where(row == 0)[0]\n",
    "        \n",
    "        for x in zero_positions:\n",
    "            left_values = row[:x]\n",
    "            right_values = row[x+1:]\n",
    "            \n",
    "            left_labels = left_values[left_values != 0]\n",
    "            right_labels = right_values[right_values != 0]\n",
    "            \n",
    "            if x >= mid_point:  # In right half of image\n",
    "                if len(left_labels) > 0:\n",
    "                    filled_slice[y, x] = left_labels[-1]\n",
    "                elif len(right_labels) > 0:\n",
    "                    filled_slice[y, x] = right_labels[0]\n",
    "            else:  # In left half of image\n",
    "                if len(right_labels) > 0:\n",
    "                    filled_slice[y, x] = right_labels[0]\n",
    "                elif len(left_labels) > 0:\n",
    "                    filled_slice[y, x] = left_labels[-1]\n",
    "    \n",
    "    return filled_slice\n",
    "\n",
    "def fill_vertical_zeros(slice_2d):\n",
    "    \"\"\"\n",
    "    Fill remaining zeros in each column with nearest non-zero value from above.\n",
    "    \"\"\"\n",
    "    filled_slice = slice_2d.copy()\n",
    "    y_dim, x_dim = slice_2d.shape\n",
    "    \n",
    "    # Process each column\n",
    "    for x in range(x_dim):\n",
    "        # Find zero positions in this column\n",
    "        column = slice_2d[:, x]\n",
    "        zero_positions = np.where(column == 0)[0]\n",
    "        \n",
    "        for y in zero_positions:\n",
    "            # Look at values above this position\n",
    "            values_above = column[:y]\n",
    "            non_zero_above = values_above[values_above != 0]\n",
    "            \n",
    "            if len(non_zero_above) > 0:\n",
    "                # Fill with nearest non-zero value from above\n",
    "                filled_slice[y, x] = non_zero_above[-1]\n",
    "    \n",
    "    return filled_slice\n",
    "\n",
    "def reverse_DAPI_cleanup(volume, min_volume_thresholds=None):\n",
    "    \"\"\"\n",
    "    Clean up DAPI volume starting from highest label, \n",
    "    keeping only largest components above threshold,\n",
    "    then fill zero islands horizontally and vertically.\n",
    "    \"\"\"\n",
    "    if min_volume_thresholds is None:\n",
    "        min_volume_thresholds = {label: 90000 for label in range(1, 8)}\n",
    "    \n",
    "    # Process labels in reverse order\n",
    "    cleaned_volume = volume.copy()\n",
    "    for label in range(7, 0, -1):\n",
    "        print(f\"Processing label {label}...\")\n",
    "        cleaned_volume = extract_largest_component(\n",
    "            cleaned_volume, \n",
    "            label, \n",
    "            min_volume_thresholds[label]\n",
    "        )\n",
    "    \n",
    "    print(\"Filling zero islands horizontally...\")\n",
    "    z_dim, y_dim, x_dim = cleaned_volume.shape\n",
    "    for z in range(z_dim):\n",
    "        if z % 10 == 0:\n",
    "            print(f\"Processing slice {z}/{z_dim}\")\n",
    "        cleaned_volume[z] = fill_horizontal_zeros(cleaned_volume[z])\n",
    "    \n",
    "    print(\"Filling remaining zeros vertically...\")\n",
    "    for z in range(z_dim):\n",
    "        if z % 10 == 0:\n",
    "            print(f\"Processing slice {z}/{z_dim}\")\n",
    "        cleaned_volume[z] = fill_vertical_zeros(cleaned_volume[z])\n",
    "    \n",
    "    return cleaned_volume\n",
    "\n",
    "def check_volume_stats(volume):\n",
    "    \"\"\"\n",
    "    Print statistics about label volumes.\n",
    "    \"\"\"\n",
    "    unique_labels, counts = np.unique(volume, return_counts=True)\n",
    "    total_voxels = volume.size\n",
    "    \n",
    "    print(\"\\nLabel Statistics:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        percentage = (count / total_voxels) * 100\n",
    "        print(f\"Label {label}: {count:,} voxels ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbd7cc63-957b-428d-a365-8fb5b8ed4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dapi_cleanup_paths(input_dir, path_to_look_at_to_run_onlyfor_those):\n",
    "    # Initialize empty lists for input and result paths\n",
    "    dapi_cleanup_input_paths = []\n",
    "    dapi_cleanup_result_paths = []\n",
    "    dapi_xy_corrected_results_paths = []\n",
    "    \n",
    "    # Convert input directory to Path object\n",
    "    input_path = Path(input_dir)\n",
    "    lookup_path = Path(path_to_look_at_to_run_onlyfor_those)\n",
    "    \n",
    "    # Get list of folder names from the lookup path\n",
    "    folders_to_process = set()\n",
    "    for item in lookup_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            folders_to_process.add(item.name)\n",
    "    \n",
    "    print(\"\\nFolders found in lookup path:\", folders_to_process)\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        root_path = Path(root)\n",
    "        \n",
    "        # Check if this is a DAPI_results directory\n",
    "        if root_path.name == \"DAPI_results\":\n",
    "            parent_folder_name = root_path.parent.name\n",
    "            \n",
    "            # Check if parent folder name is in our target list\n",
    "            if parent_folder_name not in folders_to_process:\n",
    "                #print(f\"\\nSkipped - Folder not in lookup list: {parent_folder_name}\")\n",
    "                #print(f\"Path: {root_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Check files existence\n",
    "            has_original = \"C4-DAPI-XZ_reconstructed.tif\" in files\n",
    "            has_cleaned = \"C4-DAPI-XZ_reconstructed_cleaned.tif\" in files\n",
    "\n",
    "            \"\"\"\n",
    "            if not has_original:\n",
    "                #print(f\"\\nSkipped - Original file missing in: {parent_folder_name}\")\n",
    "                #print(f\"Path: {root_path}\")\n",
    "                continue\n",
    "                \n",
    "            if has_cleaned:\n",
    "                print(f\"\\nSkipped - Cleaned file already exists in: {parent_folder_name}\")\n",
    "                print(f\"Path: {root_path}\")\n",
    "                continue\n",
    "            \"\"\"\n",
    "            \n",
    "            # If we get here, we're processing this path\n",
    "            #print(f\"\\nProcessing: {parent_folder_name}\")\n",
    "            #print(f\"Path: {root_path}\")\n",
    "            \n",
    "            # Create full path for input file\n",
    "            input_file_path = root_path / \"C4-DAPI-XZ_reconstructed.tif\"\n",
    "            #input_file_path = root_path / \"C4-DAPI-XZ_reconstructed_original_before_shape_adjustment.tif\"\n",
    "            # Create full path for output file in the same folder\n",
    "            result_file_path = root_path / \"C4-DAPI-XZ_reconstructed_cleaned.tif\"\n",
    "            # xy correction\n",
    "            xy_corrected_path = root_path / \"C4-DAPI-XZ_reconstructed_cleaned_xy.tif\"\n",
    "            \n",
    "            # Append paths to respective lists\n",
    "            dapi_cleanup_input_paths.append(str(input_file_path))\n",
    "            dapi_cleanup_result_paths.append(str(result_file_path))\n",
    "            dapi_xy_corrected_results_paths.append(str(xy_corrected_path))\n",
    "    \n",
    "    # Print summary at the end\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total paths to process: {len(dapi_cleanup_input_paths)}\")\n",
    "    \n",
    "    return dapi_cleanup_input_paths, dapi_cleanup_result_paths, dapi_xy_corrected_results_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1311ff0f-fc7e-4f86-bec5-786393e9af65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folders found in lookup path: {'DL6-2-23-21_Lu-T39ExB_M_AcquisitionBlock2_series1'}\n",
      "\n",
      "Summary:\n",
      "Total paths to process: 1\n",
      "['/research/sharedresources/cbi/common/Krishnan/Sickle_cell_git_repo/sample_test_Data/Output/DL6-2-23-21_Lu-T39ExB_M_AcquisitionBlock2_series1/DAPI_results/C4-DAPI-XZ_reconstructed.tif']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dapi_cleanup_input_paths, dapi_cleanup_result_paths, dapi_xy_corrected_results_paths = process_dapi_cleanup_paths(output_path,input_path)\n",
    "print(dapi_cleanup_input_paths)\n",
    "print(len(dapi_cleanup_input_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eef4165-a6c6-4fd4-a5e3-ac3871f4a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xz_to_xy(volume):\n",
    "    return np.transpose(volume, (1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41218e13-905d-4782-a0f9-5d2a5062753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volume reading complete ...\n",
      "Processing label 7...\n",
      "Processing label 6...\n",
      "Processing label 5...\n",
      "Processing label 4...\n",
      "Processing label 3...\n",
      "Processing label 2...\n",
      "Processing label 1...\n",
      "Filling zero islands horizontally...\n",
      "Processing slice 0/819\n",
      "Processing slice 10/819\n",
      "Processing slice 20/819\n",
      "Processing slice 30/819\n",
      "Processing slice 40/819\n",
      "Processing slice 50/819\n",
      "Processing slice 60/819\n",
      "Processing slice 70/819\n",
      "Processing slice 80/819\n",
      "Processing slice 90/819\n",
      "Processing slice 100/819\n",
      "Processing slice 110/819\n",
      "Processing slice 120/819\n",
      "Processing slice 130/819\n",
      "Processing slice 140/819\n",
      "Processing slice 150/819\n",
      "Processing slice 160/819\n",
      "Processing slice 170/819\n",
      "Processing slice 180/819\n",
      "Processing slice 190/819\n",
      "Processing slice 200/819\n",
      "Processing slice 210/819\n",
      "Processing slice 220/819\n",
      "Processing slice 230/819\n",
      "Processing slice 240/819\n",
      "Processing slice 250/819\n",
      "Processing slice 260/819\n",
      "Processing slice 270/819\n",
      "Processing slice 280/819\n",
      "Processing slice 290/819\n",
      "Processing slice 300/819\n",
      "Processing slice 310/819\n",
      "Processing slice 320/819\n",
      "Processing slice 330/819\n",
      "Processing slice 340/819\n",
      "Processing slice 350/819\n",
      "Processing slice 360/819\n",
      "Processing slice 370/819\n",
      "Processing slice 380/819\n",
      "Processing slice 390/819\n",
      "Processing slice 400/819\n",
      "Processing slice 410/819\n",
      "Processing slice 420/819\n",
      "Processing slice 430/819\n",
      "Processing slice 440/819\n",
      "Processing slice 450/819\n",
      "Processing slice 460/819\n",
      "Processing slice 470/819\n",
      "Processing slice 480/819\n",
      "Processing slice 490/819\n",
      "Processing slice 500/819\n",
      "Processing slice 510/819\n",
      "Processing slice 520/819\n",
      "Processing slice 530/819\n",
      "Processing slice 540/819\n",
      "Processing slice 550/819\n",
      "Processing slice 560/819\n",
      "Processing slice 570/819\n",
      "Processing slice 580/819\n",
      "Processing slice 590/819\n",
      "Processing slice 600/819\n",
      "Processing slice 610/819\n",
      "Processing slice 620/819\n",
      "Processing slice 630/819\n",
      "Processing slice 640/819\n",
      "Processing slice 650/819\n",
      "Processing slice 660/819\n",
      "Processing slice 670/819\n",
      "Processing slice 680/819\n",
      "Processing slice 690/819\n",
      "Processing slice 700/819\n",
      "Processing slice 710/819\n",
      "Processing slice 720/819\n",
      "Processing slice 730/819\n",
      "Processing slice 740/819\n",
      "Processing slice 750/819\n",
      "Processing slice 760/819\n",
      "Processing slice 770/819\n",
      "Processing slice 780/819\n",
      "Processing slice 790/819\n",
      "Processing slice 800/819\n",
      "Processing slice 810/819\n",
      "Filling remaining zeros vertically...\n",
      "Processing slice 0/819\n",
      "Processing slice 10/819\n",
      "Processing slice 20/819\n",
      "Processing slice 30/819\n",
      "Processing slice 40/819\n",
      "Processing slice 50/819\n",
      "Processing slice 60/819\n",
      "Processing slice 70/819\n",
      "Processing slice 80/819\n",
      "Processing slice 90/819\n",
      "Processing slice 100/819\n",
      "Processing slice 110/819\n",
      "Processing slice 120/819\n",
      "Processing slice 130/819\n",
      "Processing slice 140/819\n",
      "Processing slice 150/819\n",
      "Processing slice 160/819\n",
      "Processing slice 170/819\n",
      "Processing slice 180/819\n",
      "Processing slice 190/819\n",
      "Processing slice 200/819\n",
      "Processing slice 210/819\n",
      "Processing slice 220/819\n",
      "Processing slice 230/819\n",
      "Processing slice 240/819\n",
      "Processing slice 250/819\n",
      "Processing slice 260/819\n",
      "Processing slice 270/819\n",
      "Processing slice 280/819\n",
      "Processing slice 290/819\n",
      "Processing slice 300/819\n",
      "Processing slice 310/819\n",
      "Processing slice 320/819\n",
      "Processing slice 330/819\n",
      "Processing slice 340/819\n",
      "Processing slice 350/819\n",
      "Processing slice 360/819\n",
      "Processing slice 370/819\n",
      "Processing slice 380/819\n",
      "Processing slice 390/819\n",
      "Processing slice 400/819\n",
      "Processing slice 410/819\n",
      "Processing slice 420/819\n",
      "Processing slice 430/819\n",
      "Processing slice 440/819\n",
      "Processing slice 450/819\n",
      "Processing slice 460/819\n",
      "Processing slice 470/819\n",
      "Processing slice 480/819\n",
      "Processing slice 490/819\n",
      "Processing slice 500/819\n",
      "Processing slice 510/819\n",
      "Processing slice 520/819\n",
      "Processing slice 530/819\n",
      "Processing slice 540/819\n",
      "Processing slice 550/819\n",
      "Processing slice 560/819\n",
      "Processing slice 570/819\n",
      "Processing slice 580/819\n",
      "Processing slice 590/819\n",
      "Processing slice 600/819\n",
      "Processing slice 610/819\n",
      "Processing slice 620/819\n",
      "Processing slice 630/819\n",
      "Processing slice 640/819\n",
      "Processing slice 650/819\n",
      "Processing slice 660/819\n",
      "Processing slice 670/819\n",
      "Processing slice 680/819\n",
      "Processing slice 690/819\n",
      "Processing slice 700/819\n",
      "Processing slice 710/819\n",
      "Processing slice 720/819\n",
      "Processing slice 730/819\n",
      "Processing slice 740/819\n",
      "Processing slice 750/819\n",
      "Processing slice 760/819\n",
      "Processing slice 770/819\n",
      "Processing slice 780/819\n",
      "Processing slice 790/819\n",
      "Processing slice 800/819\n",
      "Processing slice 810/819\n",
      "Cleanup successful\n"
     ]
    }
   ],
   "source": [
    "for tif_file_path, results_path, xy_corrected_results_path in zip(dapi_cleanup_input_paths, dapi_cleanup_result_paths, dapi_xy_corrected_results_paths):\n",
    "    volume = tifffile.imread(tif_file_path)\n",
    "    print(\"volume reading complete ...\")\n",
    "    cleaned_volume = reverse_DAPI_cleanup(volume)\n",
    "    print(\"Cleanup successful\")\n",
    "    tifffile.imwrite(results_path, cleaned_volume)\n",
    "    xy_final_volume = convert_xz_to_xy(cleaned_volume)\n",
    "    tifffile.imwrite(xy_corrected_results_path, xy_final_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de82382-8228-4fba-a6b7-6f115c7e4e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6461b-810b-44e1-8cd1-67ca131010fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
